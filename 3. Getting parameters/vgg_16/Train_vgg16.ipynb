{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import scipy.ndimage\n",
    "from scipy import misc\n",
    "\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "import numpy as np\n",
    "import numpy.matlib as ml\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import scipy.io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/fedor/.conda/envs/tf-gpu/Lib/site-packages/tensorflow/contrib/slim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the job...\n"
     ]
    }
   ],
   "source": [
    "print(\"starting the job...\")\n",
    "\n",
    "num_out = 5    #number ouf output parameters being predicted\n",
    "\n",
    "global numpix_side \n",
    "numpix_side = 192   #number of image pixels on the side\n",
    "\n",
    "global max_noise_rms, max_psf_rms , max_cr_intensity\n",
    "max_trainoise_rms = 0.1 # maximum rms of noise in training data\n",
    "max_testnoise_rms = 0.1 # maximum rms of noise in test or validation data\n",
    "max_noise_rms = max_testnoise_rms\n",
    "\n",
    "max_psf_rms = 0.08/0.04  # maximum Gaussian PSF rms (in pixels)\n",
    "max_cr_intensity = 0.5 # maximum scaling for cosmic ray and artefact maps\n",
    "\n",
    "global constant_noise_rms\n",
    "variable_noise_rms = True  #if True, the noise rms will be chosen randomly for each sample with a max of max_noise_rms (above)\n",
    "\n",
    "\n",
    "\n",
    "global pix_res\n",
    "pix_res = 0.04 # pixel size in arcsec\n",
    "L_side = pix_res * numpix_side\n",
    "\n",
    "global arcs_data_path_1, arcs_data_path_2 , test_data_path_1 , test_data_path_2 , CRay_data_path\n",
    "global lens_data_path_1, lens_data_path_2, testlens_data_path_1, testlens_data_path_2 \n",
    "\n",
    "global min_unmasked_flux \n",
    "min_unmasked_flux = 0.75\n",
    "\n",
    "#number of folders containing training or test data. If all 3 point to the same folder that's OK (only that folder will be used).\n",
    "global num_data_dirs\n",
    "num_data_dirs = 3\n",
    "\n",
    "num_training_samples = 79992\n",
    "\n",
    "PATH_train = 'D:/'\n",
    "PATH_test = 'D:/'\n",
    "\n",
    "arcs_data_path_1 = PATH_train + 'lensed_images/'\n",
    "arcs_data_path_2 = arcs_data_path_1\n",
    "arcs_data_path_3 = arcs_data_path_1\n",
    "\n",
    "test_data_path_1 = PATH_test + 'lensed_images_test/'\n",
    "test_data_path_2 = test_data_path_1\n",
    "test_data_path_3 = test_data_path_1\n",
    "\n",
    "lens_data_path_1 = PATH_train + 'lensed_images/'\n",
    "lens_data_path_2 = lens_data_path_1\n",
    "lens_data_path_3 = lens_data_path_1\n",
    "\n",
    "testlens_data_path_1 = PATH_test + 'lensed_images_test/'\n",
    "testlens_data_path_2 = testlens_data_path_1\n",
    "testlens_data_path_3 = testlens_data_path_1\n",
    "\n",
    "#folder containing cosmic rays\n",
    "CRay_data_path   = '../../data_Ensai/CosmicRays/'\n",
    "\n",
    "global max_xy_range   # xy range of center of the lens. The image is shifted in a central area with a side of max_xy_range (arcsec) during training or testing\n",
    "max_xy_range = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(\"get_data.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% MODEL DEFINITION\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, numpix_side*numpix_side])   #placeholder for input image\n",
    "y_ = tf.placeholder(tf.float32, shape=[None,num_out])    #placeholder for output parameters during training\n",
    "x_image0 = tf.reshape(x, [-1,numpix_side,numpix_side,1])\n",
    "\n",
    "# removing image intensity bias: filter image with a 4X4 filter and remove from image\n",
    "MASK = tf.abs(tf.sign(x_image0))\n",
    "XX =  x_image0 +  ( (1-MASK) * 1000.0)\n",
    "bias_measure_filt = tf.constant((1.0/16.0), shape=[4, 4, 1, 1])\n",
    "bias_measure = tf.nn.conv2d( XX , bias_measure_filt , strides=[1, 1, 1, 1], padding='VALID')\n",
    "im_bias = tf.reshape( tf.reduce_min(bias_measure,axis=[1,2,3]) , [-1,1,1,1] )\n",
    "x_image = x_image0 - (im_bias * MASK )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct models\n",
    "exec(open(\"ensai_model.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0A6CE48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0A6CE48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0A6CE48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0A6CE48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E063D8C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E063D8C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E063D8C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E063D8C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213DEB4C908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213DEB4C908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213DEB4C908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213DEB4C908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213D8BAC708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213D8BAC708>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213D8BAC708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213D8BAC708>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E063DB88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E063DB88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E063DB88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E063DB88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213D8B6FFC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213D8B6FFC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213D8B6FFC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213D8B6FFC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DFD7E0C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DFD7E0C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DFD7E0C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DFD7E0C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF334EC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF334EC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF334EC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF334EC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF278788>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF278788>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF278788>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF278788>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213C6E79C08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213C6E79C08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213C6E79C08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213C6E79C08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF29B248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF29B248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF29B248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF29B248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0664E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0664E48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0664E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0664E48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DEC091C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DEC091C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DEC091C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DEC091C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213D8BAC708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213D8BAC708>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213D8BAC708>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213D8BAC708>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF353448>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF353448>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF353448>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF353448>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DEC00148>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DEC00148>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DEC00148>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DEC00148>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DCA12508>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DCA12508>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DCA12508>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DCA12508>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213DEBB8208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213DEBB8208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213DEBB8208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000213DEBB8208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF29B248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF29B248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF29B248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF29B248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x00000213DFD93488>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x00000213DFD93488>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x00000213DFD93488>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x00000213DFD93488>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E065F808>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E065F808>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E065F808>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E065F808>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x00000213DF353448>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x00000213DF353448>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x00000213DF353448>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x00000213DF353448>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF2E2B48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF2E2B48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF2E2B48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DF2E2B48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"ENSAI\"):\n",
    "    y_out = vgg_16(x_image,scope=\"vgg_16\", is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_save =  slim.get_variables(scope=\"ENSAI/vgg_16\")   #list of variables to save\n",
    "train_pars = variables_to_save  #list of parameters to train\n",
    "variables_to_restore = variables_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = './ckpt/vgg_16_second.ckpt'\n",
    "restore_file = './ckpt/vgg_16.ckpt'   #path of network weights file to restore from\n",
    "\n",
    "RESTORE = True\n",
    "SAVE = True\n",
    "restorer = tf.train.Saver(variables_to_restore)\n",
    "saver = tf.train.Saver(variables_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## flipping and cost function\n",
    "MeanSquareCost , y_out_flipped = cost_tensor(y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "############### OPTIMIZER:\n",
    "learning_rate = 0.1\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(MeanSquareCost,var_list=train_pars)\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_eval_cost = 0.05\n",
    "cycle_batch_size = 8\n",
    "X = np.zeros((cycle_batch_size,numpix_side*numpix_side), dtype='float32') ;\n",
    "Y = np.zeros((cycle_batch_size,num_out), dtype='float32' );\n",
    "MAG = np.zeros((cycle_batch_size,1), dtype='float32' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 200 # number of test samples\n",
    "max_num_test_samples = 10003\n",
    "\n",
    "X_test = np.zeros((num_test_samples,numpix_side*numpix_side), dtype='float32'  );\n",
    "Y_test = np.zeros((num_test_samples,num_out), dtype='float32' );\n",
    "MAG_test = np.zeros((num_test_samples,1), dtype='float32' );\n",
    "Predictions = np.zeros((num_test_samples , num_out ) , dtype='float32' )\n",
    "# max_noise_rms = max_testnoise_rms\n",
    "\n",
    "ind_t = range(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_batch(X_test, Y_test, MAG_test, max_num_test_samples, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fedor\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt/vgg_16.ckpt\n"
     ]
    }
   ],
   "source": [
    "opts = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=opts))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if RESTORE:\n",
    "    restorer.restore(sess, restore_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = '_vgg_16'\n",
    "num_iterations = int(num_training_samples/cycle_batch_size)\n",
    "max_file_num = 79991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79992"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = open(\"log_file.txt\",\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_vgg_16, lr: 0.1, [0.79 0.35 0.38 0.16 0.16]\n",
      "                                         0000  0.00000 0.57562    0.56254    0.05000   31.289\n",
      "mod_vgg_16, lr: 0.1, [0.79 0.36 0.25 0.16 0.16]\n",
      "                                         0000  75.00000 0.12213    0.17653    0.05000   152.738\n",
      "mod_vgg_16, lr: 0.1, [0.79 0.28 0.33 0.16 0.16]\n",
      "                                         0000  150.00000 0.18423    0.17601    0.05000   152.676\n",
      "mod_vgg_16, lr: 0.1, [0.78 0.35 0.24 0.16 0.16]\n",
      "                                         0000  225.00000 0.07417    0.17226    0.05000   152.957\n",
      "mod_vgg_16, lr: 0.1, [0.77 0.32 0.32 0.16 0.16]\n",
      "                                         0000  300.00000 0.10563    0.17862    0.05000   152.702\n",
      "mod_vgg_16, lr: 0.1, [0.39 0.25 0.34 0.16 0.16]\n",
      "                                         0000  375.00000 0.08501    0.07778    0.05000   152.919\n",
      "mod_vgg_16, lr: 0.1, [0.29 0.3  0.29 0.16 0.15]\n",
      "                                         0000  450.00000 0.05637    0.06353    0.05000   152.750\n",
      "mod_vgg_16, lr: 0.1, [0.24 0.28 0.27 0.16 0.15]\n",
      "                                         0000  525.00000 0.03054    0.05378    0.05000   152.807\n",
      "mod_vgg_16, lr: 0.1, [0.22 0.25 0.28 0.15 0.15]\n",
      "                                         0000  600.00000 0.02728    0.04931    0.05000   152.813\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.23 0.24 0.27 0.15 0.15]\n",
      "                                         0000  675.00000 0.03993    0.04709    0.04931   155.279\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.19 0.25 0.23 0.15 0.14]\n",
      "                                         0000  750.00000 0.01446    0.04055    0.04709   155.153\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.2  0.24 0.22 0.15 0.14]\n",
      "                                         0000  825.00000 0.01937    0.04086    0.04055   155.575\n",
      "mod_vgg_16, lr: 0.1, [0.17 0.21 0.21 0.15 0.15]\n",
      "                                         0000  900.00000 0.02603    0.03304    0.04055   154.326\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.16 0.22 0.2  0.14 0.14]\n",
      "                                         0000  975.00000 0.02117    0.03337    0.03304   155.510\n",
      "mod_vgg_16, lr: 0.1, [0.17 0.23 0.19 0.14 0.14]\n",
      "                                         0000  1050.00000 0.02265    0.03327    0.03304   154.658\n",
      "mod_vgg_16, lr: 0.1, [0.16 0.21 0.21 0.14 0.14]\n",
      "                                         0000  1125.00000 0.01322    0.03323    0.03304   154.611\n",
      "mod_vgg_16, lr: 0.1, [0.14 0.19 0.21 0.14 0.14]\n",
      "                                         0000  1200.00000 0.01615    0.02895    0.03304   154.783\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.14 0.2  0.2  0.14 0.14]\n",
      "                                         0000  1275.00000 0.01025    0.03362    0.02895   155.443\n",
      "mod_vgg_16, lr: 0.1, [0.13 0.2  0.19 0.14 0.14]\n",
      "                                         0000  1350.00000 0.01412    0.02770    0.02895   154.361\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.12 0.21 0.18 0.14 0.13]\n",
      "                                         0000  1425.00000 0.02255    0.02707    0.02770   156.017\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.13 0.2  0.18 0.13 0.13]\n",
      "                                         0000  1500.00000 0.01359    0.02626    0.02707   155.707\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.14 0.22 0.19 0.14 0.14]\n",
      "                                         0000  1575.00000 0.01020    0.02913    0.02626   159.096\n",
      "mod_vgg_16, lr: 0.1, [0.13 0.2  0.21 0.13 0.13]\n",
      "                                         0000  1650.00000 0.02045    0.02745    0.02626   157.464\n",
      "mod_vgg_16, lr: 0.1, [0.14 0.18 0.17 0.13 0.13]\n",
      "                                         0000  1725.00000 0.01337    0.02406    0.02626   155.359\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.12 0.2  0.22 0.13 0.14]\n",
      "                                         0000  1800.00000 0.00974    0.02844    0.02406   156.639\n",
      "mod_vgg_16, lr: 0.1, [0.12 0.16 0.17 0.14 0.13]\n",
      "                                         0000  1875.00000 0.01629    0.02494    0.02406   155.798\n",
      "mod_vgg_16, lr: 0.1, [0.12 0.19 0.17 0.13 0.13]\n",
      "                                         0000  1950.00000 0.01540    0.02505    0.02406   156.007\n",
      "mod_vgg_16, lr: 0.1, [0.11 0.16 0.17 0.13 0.13]\n",
      "                                         0000  2025.00000 0.00692    0.02055    0.02406   157.698\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.1  0.16 0.16 0.13 0.13]\n",
      "                                         0000  2100.00000 0.00451    0.02062    0.02055   156.905\n",
      "mod_vgg_16, lr: 0.1, [0.11 0.17 0.17 0.12 0.13]\n",
      "                                         0000  2175.00000 0.00856    0.02081    0.02055   156.684\n",
      "mod_vgg_16, lr: 0.1, [0.1  0.17 0.17 0.13 0.13]\n",
      "                                         0000  2250.00000 0.00516    0.02104    0.02055   155.319\n",
      "mod_vgg_16, lr: 0.1, [0.1  0.21 0.19 0.13 0.13]\n",
      "                                         0000  2325.00000 0.01351    0.02559    0.02055   156.552\n",
      "mod_vgg_16, lr: 0.1, [0.1  0.15 0.16 0.12 0.13]\n",
      "                                         0000  2400.00000 0.01304    0.01890    0.02055   155.556\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.16 0.15 0.12 0.12]\n",
      "                                         0000  2475.00000 0.00586    0.01820    0.01890   156.535\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.11 0.17 0.17 0.12 0.13]\n",
      "                                         0000  2550.00000 0.01357    0.02097    0.01820   156.435\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.15 0.16 0.12 0.13]\n",
      "                                         0000  2625.00000 0.00742    0.01873    0.01820   155.004\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.18 0.16 0.12 0.12]\n",
      "                                         0000  2700.00000 0.00528    0.01938    0.01820   155.426\n",
      "mod_vgg_16, lr: 0.1, [0.1  0.16 0.17 0.11 0.12]\n",
      "                                         0000  2775.00000 0.01018    0.01972    0.01820   155.072\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.16 0.16 0.12 0.13]\n",
      "                                         0000  2850.00000 0.00784    0.01818    0.01820   155.051\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.1  0.16 0.15 0.11 0.12]\n",
      "                                         0000  2925.00000 0.01424    0.01812    0.01818   156.099\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.1  0.15 0.15 0.11 0.12]\n",
      "                                         0000  3000.00000 0.00398    0.01715    0.01812   155.971\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.1  0.14 0.16 0.11 0.12]\n",
      "                                         0000  3075.00000 0.00569    0.01688    0.01715   156.357\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.15 0.18 0.12 0.11]\n",
      "                                         0000  3150.00000 0.00286    0.01810    0.01688   156.353\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.19 0.17 0.11 0.12]\n",
      "                                         0000  3225.00000 0.00582    0.02100    0.01688   155.517\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.17 0.15 0.11 0.12]\n",
      "                                         0000  3300.00000 0.00405    0.01886    0.01688   155.999\n",
      "mod_vgg_16, lr: 0.1, [0.1  0.14 0.15 0.12 0.11]\n",
      "                                         0000  3375.00000 0.00555    0.01664    0.01688   155.868\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.11 0.15 0.16 0.12 0.12]\n",
      "                                         0000  3450.00000 0.00734    0.01781    0.01664   156.075\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.13 0.17 0.11 0.11]\n",
      "                                         0000  3525.00000 0.00282    0.01670    0.01664   155.252\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.16 0.16 0.11 0.12]\n",
      "                                         0000  3600.00000 0.00580    0.01780    0.01664   155.243\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.14 0.14 0.11 0.11]\n",
      "                                         0000  3675.00000 0.00627    0.01487    0.01664   155.477\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.16 0.16 0.11 0.12]\n",
      "                                         0000  3750.00000 0.00807    0.01814    0.01487   157.156\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.15 0.15 0.11 0.12]\n",
      "                                         0000  3825.00000 0.00999    0.01811    0.01487   156.270\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.15 0.16 0.11 0.12]\n",
      "                                         0000  3900.00000 0.00248    0.01693    0.01487   156.197\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.15 0.15 0.11 0.12]\n",
      "                                         0000  3975.00000 0.00383    0.01686    0.01487   156.563\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.17 0.15 0.11 0.11]\n",
      "                                         0000  4050.00000 0.00834    0.01733    0.01487   156.216\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.14 0.15 0.11 0.11]\n",
      "                                         0000  4125.00000 0.00431    0.01528    0.01487   156.031\n",
      "mod_vgg_16, lr: 0.1, [0.1  0.15 0.15 0.11 0.11]\n",
      "                                         0000  4200.00000 0.00549    0.01695    0.01487   155.657\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.14 0.14 0.11 0.11]\n",
      "                                         0000  4275.00000 0.00361    0.01515    0.01487   155.910\n",
      "mod_vgg_16, lr: 0.1, [0.1  0.13 0.15 0.11 0.11]\n",
      "                                         0000  4350.00000 0.00609    0.01594    0.01487   155.818\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.13 0.16 0.11 0.11]\n",
      "                                         0000  4425.00000 0.00740    0.01521    0.01487   155.916\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.14 0.14 0.11 0.11]\n",
      "                                         0000  4500.00000 0.00887    0.01518    0.01487   155.464\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.16 0.15 0.11 0.11]\n",
      "                                         0000  4575.00000 0.00884    0.01692    0.01487   155.387\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.14 0.11 0.11]\n",
      "                                         0000  4650.00000 0.00676    0.01409    0.01487   156.093\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.15 0.15 0.11 0.11]\n",
      "                                         0000  4725.00000 0.00391    0.01587    0.01409   156.707\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.14 0.15 0.1  0.1 ]\n",
      "                                         0000  4800.00000 0.00403    0.01465    0.01409   156.000\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.14 0.14 0.1  0.1 ]\n",
      "                                         0000  4875.00000 0.00520    0.01408    0.01409   156.615\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.14 0.16 0.11 0.11]\n",
      "                                         0000  4950.00000 0.00435    0.01560    0.01408   156.994\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.14 0.15 0.1  0.11]\n",
      "                                         0000  5025.00000 0.00205    0.01471    0.01408   156.170\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.14 0.15 0.11 0.11]\n",
      "                                         0000  5100.00000 0.00301    0.01494    0.01408   156.649\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.18 0.16 0.1  0.11]\n",
      "                                         0000  5175.00000 0.00657    0.01910    0.01408   156.446\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.14 0.15 0.11 0.11]\n",
      "                                         0000  5250.00000 0.00430    0.01474    0.01408   156.210\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.15 0.15 0.1  0.1 ]\n",
      "                                         0000  5325.00000 0.00191    0.01508    0.01408   156.195\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.14 0.15 0.1  0.1 ]\n",
      "                                         0000  5400.00000 0.00304    0.01492    0.01408   156.331\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.14 0.11 0.1 ]\n",
      "                                         0000  5475.00000 0.00571    0.01428    0.01408   156.425\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.14 0.15 0.1  0.11]\n",
      "                                         0000  5550.00000 0.00913    0.01547    0.01408   156.762\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.13 0.13 0.1  0.1 ]\n",
      "                                         0000  5625.00000 0.01385    0.01284    0.01408   156.177\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.13 0.1  0.11]\n",
      "                                         0000  5700.00000 0.00172    0.01268    0.01284   157.664\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.15 0.15 0.11 0.1 ]\n",
      "                                         0000  5775.00000 0.00253    0.01506    0.01268   157.457\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.16 0.1  0.1 ]\n",
      "                                         0000  5850.00000 0.00461    0.01473    0.01268   155.800\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.15 0.13 0.1  0.11]\n",
      "                                         0000  5925.00000 0.00729    0.01384    0.01268   156.348\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.13 0.1  0.1 ]\n",
      "                                         0000  6000.00000 0.00214    0.01229    0.01268   156.645\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.14 0.14 0.1  0.11]\n",
      "                                         0000  6075.00000 0.00254    0.01384    0.01229   157.456\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.14 0.14 0.1  0.11]\n",
      "                                         0000  6150.00000 0.00428    0.01364    0.01229   159.657\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.14 0.1  0.11]\n",
      "                                         0000  6225.00000 0.00250    0.01367    0.01229   156.891\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.14 0.1  0.1 ]\n",
      "                                         0000  6300.00000 0.00233    0.01266    0.01229   157.336\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.14 0.1  0.1 ]\n",
      "                                         0000  6375.00000 0.00602    0.01336    0.01229   157.012\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.12 0.14 0.1  0.1 ]\n",
      "                                         0000  6450.00000 0.00414    0.01234    0.01229   156.540\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.14 0.1  0.1 ]\n",
      "                                         0000  6525.00000 0.00600    0.01255    0.01229   156.262\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.14 0.13 0.1  0.1 ]\n",
      "                                         0000  6600.00000 0.00100    0.01277    0.01229   156.234\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.13 0.1  0.1 ]\n",
      "                                         0000  6675.00000 0.00580    0.01260    0.01229   156.208\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.14 0.09 0.1 ]\n",
      "                                         0000  6750.00000 0.00203    0.01303    0.01229   155.978\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.14 0.1  0.1 ]\n",
      "                                         0000  6825.00000 0.00551    0.01262    0.01229   156.195\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.14 0.1  0.1 ]\n",
      "                                         0000  6900.00000 0.00212    0.01345    0.01229   156.296\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.14 0.09 0.1 ]\n",
      "                                         0000  6975.00000 0.00115    0.01259    0.01229   156.267\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.11 0.14 0.09 0.1 ]\n",
      "                                         0000  7050.00000 0.01028    0.01148    0.01229   156.044\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.13 0.13 0.1  0.1 ]\n",
      "                                         0000  7125.00000 0.00328    0.01237    0.01148   157.716\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.13 0.09 0.09]\n",
      "                                         0000  7200.00000 0.00226    0.01109    0.01148   155.735\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.12 0.15 0.1  0.1 ]\n",
      "                                         0000  7275.00000 0.00399    0.01275    0.01109   161.009\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.13 0.14 0.1  0.09]\n",
      "                                         0000  7350.00000 0.00508    0.01223    0.01109   158.768\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.12 0.15 0.1  0.1 ]\n",
      "                                         0000  7425.00000 0.00236    0.01337    0.01109   157.050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-2be113481873>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mread_data_batch\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mMAG\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmax_file_num\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m75\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "write_time = time.time()\n",
    "start_time = time.time()\n",
    "print_per = 1\n",
    "\n",
    "costs = []\n",
    "seed = 0\n",
    "\n",
    "for num_epoch in range(3):\n",
    "    for i in range(num_iterations):\n",
    "        seed += 1 \n",
    "\n",
    "        read_data_batch( X , Y , MAG , max_file_num , 'train', seed)\n",
    "        sess.run(train_step, feed_dict={x: X, y_: Y})\n",
    "\n",
    "        if i%75 == 0:     \n",
    "            train_cost = sess.run(MeanSquareCost, feed_dict={x: X, y_: Y} )\n",
    "            costs.append(train_cost)\n",
    "\n",
    "            sum_rms = 0\n",
    "            eval_cost = 0\n",
    "            chunk_size = 50\n",
    "            num_chunks = int(num_test_samples/chunk_size)\n",
    "            for it in range(num_chunks):\n",
    "                    eval_cost  = eval_cost + sess.run(MeanSquareCost, feed_dict={x: X_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it]], y_: Y_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:]})\n",
    "                    A = sess.run(y_out , feed_dict={ x: X_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it]]})\n",
    "                    B = sess.run(y_out_flipped , feed_dict={ x: X_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it]]})\n",
    "                    Predictions[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:] = get_rotation_corrected(A,B,Y_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:])\n",
    "                    sum_rms = sum_rms + np.std(Predictions[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:]- \\\n",
    "                                               Y_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:],axis=0)\n",
    "            eval_cost = eval_cost / num_chunks\n",
    "            print(\"mod\"+ str(model_num) + \", lr: \" + str(learning_rate) + \", \"  + np.array_str( sum_rms/num_chunks  ,precision=2) )\n",
    "\n",
    "\n",
    "        # show the iteration number, training cost, validation cost, and the average time per iteration for training\n",
    "            print(\"                                         %0.4d  %0.5f %0.5f    %0.5f    %0.5f   %0.3f\"%(num_epoch, i, train_cost,eval_cost,min_eval_cost,(time.time()-start_time)/print_per)) \n",
    "            start_time = time.time()\n",
    "\n",
    "            log_file = open(\"log_file.txt\",\"a\")\n",
    "            log_file.write('%d ' % (num_epoch) + ' '.join(map(str,sum_rms/num_chunks)) + ' %0.5f %0.5f\\n' % (train_cost,eval_cost) )\n",
    "            log_file.close()               \n",
    "\n",
    "            if  SAVE & (eval_cost<min_eval_cost): # save file when validation cost drops\n",
    "                    print(\"saving weights to the disk (eval) ...\")\n",
    "                    save_path = saver.save(sess, save_file)\n",
    "                    print(\"done.\")\n",
    "            min_eval_cost = np.minimum(min_eval_cost,eval_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.13 0.1  0.1 ]\n",
      "                                         0000  0.00000 0.00375    0.01174    0.01109   20.899\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.14 0.1  0.09]\n",
      "                                         0000  75.00000 0.00268    0.01197    0.01109   155.429\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.12 0.13 0.1  0.1 ]\n",
      "                                         0000  150.00000 0.00206    0.01201    0.01109   155.209\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.1  0.13 0.1  0.09]\n",
      "                                         0000  225.00000 0.00199    0.01063    0.01109   156.486\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.14 0.14 0.1  0.09]\n",
      "                                         0000  300.00000 0.01181    0.01325    0.01063   157.841\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.09]\n",
      "                                         0000  375.00000 0.00282    0.01105    0.01063   155.451\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.14 0.14 0.1  0.09]\n",
      "                                         0000  450.00000 0.00696    0.01360    0.01063   155.331\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.13 0.14 0.09 0.09]\n",
      "                                         0000  525.00000 0.00384    0.01221    0.01063   155.388\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.13 0.13 0.09 0.09]\n",
      "                                         0000  600.00000 0.00208    0.01152    0.01063   155.435\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.12 0.14 0.09 0.09]\n",
      "                                         0000  675.00000 0.00457    0.01233    0.01063   155.157\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.13 0.09 0.09]\n",
      "                                         0000  750.00000 0.00143    0.01054    0.01063   154.784\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.13 0.14 0.1  0.09]\n",
      "                                         0000  825.00000 0.00388    0.01265    0.01054   156.857\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.14 0.1  0.09]\n",
      "                                         0000  900.00000 0.00169    0.01155    0.01054   154.562\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.14 0.1  0.09]\n",
      "                                         0000  975.00000 0.00302    0.01165    0.01054   154.725\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.09]\n",
      "                                         0000  1050.00000 0.00211    0.01110    0.01054   154.656\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.14 0.1  0.1 ]\n",
      "                                         0000  1125.00000 0.00507    0.01143    0.01054   154.498\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.12 0.13 0.09 0.1 ]\n",
      "                                         0000  1200.00000 0.00282    0.01126    0.01054   154.445\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.14 0.1  0.09]\n",
      "                                         0000  1275.00000 0.00234    0.01162    0.01054   154.563\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.15 0.09 0.09]\n",
      "                                         0000  1350.00000 0.00493    0.01278    0.01054   154.378\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.1  0.09]\n",
      "                                         0000  1425.00000 0.00262    0.01118    0.01054   154.362\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.12 0.09 0.09]\n",
      "                                         0000  1500.00000 0.00196    0.01110    0.01054   154.258\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.14 0.1  0.09]\n",
      "                                         0000  1575.00000 0.00510    0.01110    0.01054   154.548\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.1  0.09]\n",
      "                                         0000  1650.00000 0.00434    0.01141    0.01054   154.418\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.1  0.14 0.1  0.09]\n",
      "                                         0000  1725.00000 0.00244    0.01127    0.01054   154.330\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.14 0.09 0.09]\n",
      "                                         0000  1800.00000 0.00258    0.01084    0.01054   154.250\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.12 0.13 0.1  0.09]\n",
      "                                         0000  1875.00000 0.00722    0.01196    0.01054   154.346\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.09]\n",
      "                                         0000  1950.00000 0.00194    0.01124    0.01054   154.328\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.12 0.14 0.09 0.09]\n",
      "                                         0000  2025.00000 0.00182    0.01114    0.01054   154.151\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.08]\n",
      "                                         0000  2100.00000 0.00301    0.01125    0.01054   154.135\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.14 0.1  0.09]\n",
      "                                         0000  2175.00000 0.00353    0.01163    0.01054   154.330\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.15 0.09 0.09]\n",
      "                                         0000  2250.00000 0.00166    0.01129    0.01054   154.235\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.12 0.14 0.1  0.09]\n",
      "                                         0000  2325.00000 0.01248    0.01147    0.01054   154.401\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.14 0.09 0.09]\n",
      "                                         0000  2400.00000 0.00259    0.01133    0.01054   154.317\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.14 0.15 0.1  0.1 ]\n",
      "                                         0000  2475.00000 0.00252    0.01384    0.01054   154.242\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.13 0.14 0.1  0.09]\n",
      "                                         0000  2550.00000 0.00187    0.01214    0.01054   154.379\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.09]\n",
      "                                         0000  2625.00000 0.00426    0.01019    0.01054   154.314\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.14 0.1  0.09]\n",
      "                                         0000  2700.00000 0.01297    0.01271    0.01019   155.229\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.14 0.09 0.09]\n",
      "                                         0000  2775.00000 0.00284    0.01113    0.01019   154.598\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.13 0.13 0.09 0.08]\n",
      "                                         0000  2850.00000 0.00413    0.01086    0.01019   154.296\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.12 0.09 0.09]\n",
      "                                         0000  2925.00000 0.00451    0.01046    0.01019   154.299\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.09]\n",
      "                                         0000  3000.00000 0.00362    0.01063    0.01019   154.417\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.13 0.09 0.09]\n",
      "                                         0000  3075.00000 0.00242    0.00990    0.01019   154.429\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.14 0.09 0.09]\n",
      "                                         0000  3150.00000 0.00116    0.01047    0.00990   155.294\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.14 0.1  0.08]\n",
      "                                         0000  3225.00000 0.00325    0.01064    0.00990   154.516\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.1  0.09]\n",
      "                                         0000  3300.00000 0.00202    0.01080    0.00990   154.505\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.13 0.09 0.09]\n",
      "                                         0000  3375.00000 0.00246    0.01055    0.00990   154.305\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.13 0.09 0.09]\n",
      "                                         0000  3450.00000 0.00534    0.00973    0.00990   154.358\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.12 0.09 0.09]\n",
      "                                         0000  3525.00000 0.00136    0.00937    0.00973   155.247\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.13 0.13 0.1  0.09]\n",
      "                                         0000  3600.00000 0.00363    0.01152    0.00937   155.219\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.09]\n",
      "                                         0000  3675.00000 0.00161    0.01023    0.00937   154.498\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.14 0.1  0.1 ]\n",
      "                                         0000  3750.00000 0.00437    0.01217    0.00937   154.364\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.12 0.09 0.09]\n",
      "                                         0000  3825.00000 0.00296    0.00982    0.00937   154.512\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.13 0.09 0.09]\n",
      "                                         0000  3900.00000 0.00233    0.01003    0.00937   154.481\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.13 0.09 0.09]\n",
      "                                         0000  3975.00000 0.00229    0.00972    0.00937   154.380\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.13 0.09 0.09]\n",
      "                                         0000  4050.00000 0.00223    0.00991    0.00937   154.307\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.12 0.09 0.09]\n",
      "                                         0000  4125.00000 0.00191    0.00986    0.00937   154.206\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.13 0.09 0.09]\n",
      "                                         0000  4200.00000 0.00295    0.00982    0.00937   154.180\n",
      "mod_vgg_16, lr: 0.1, [0.09 0.1  0.12 0.09 0.09]\n",
      "                                         0000  4275.00000 0.00304    0.01035    0.00937   154.322\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.09]\n",
      "                                         0000  4350.00000 0.00614    0.00999    0.00937   154.284\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.13 0.09 0.09]\n",
      "                                         0000  4425.00000 0.00268    0.00950    0.00937   154.178\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.12 0.09 0.09]\n",
      "                                         0000  4500.00000 0.00377    0.00965    0.00937   154.255\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.13 0.13 0.09 0.09]\n",
      "                                         0000  4575.00000 0.00248    0.01200    0.00937   154.313\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.12 0.09 0.09]\n",
      "                                         0000  4650.00000 0.00110    0.00973    0.00937   154.143\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.13 0.09 0.08]\n",
      "                                         0000  4725.00000 0.00534    0.01018    0.00937   154.133\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.11 0.11 0.09 0.08]\n",
      "                                         0000  4800.00000 0.00474    0.00920    0.00937   154.412\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.12 0.09 0.08]\n",
      "                                         0000  4875.00000 0.00502    0.01005    0.00920   154.999\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.11 0.08 0.08]\n",
      "                                         0000  4950.00000 0.00047    0.00859    0.00920   154.151\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.12 0.08 0.08]\n",
      "                                         0000  5025.00000 0.00251    0.00973    0.00859   155.340\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.13 0.09 0.09]\n",
      "                                         0000  5100.00000 0.00095    0.00993    0.00859   154.357\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.09]\n",
      "                                         0000  5175.00000 0.00132    0.00973    0.00859   154.406\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.09]\n",
      "                                         0000  5250.00000 0.00209    0.00985    0.00859   154.347\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.13 0.09 0.09]\n",
      "                                         0000  5325.00000 0.00228    0.01045    0.00859   154.205\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.14 0.09 0.08]\n",
      "                                         0000  5400.00000 0.01358    0.01133    0.00859   154.236\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.12 0.09 0.08]\n",
      "                                         0000  5475.00000 0.00488    0.00954    0.00859   154.296\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.12 0.09 0.08]\n",
      "                                         0000  5550.00000 0.00041    0.00933    0.00859   154.437\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.14 0.08 0.08]\n",
      "                                         0000  5625.00000 0.00322    0.00987    0.00859   154.441\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.1  0.12 0.08 0.08]\n",
      "                                         0000  5700.00000 0.00434    0.00924    0.00859   154.402\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.08]\n",
      "                                         0000  5775.00000 0.00212    0.00995    0.00859   154.208\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.08]\n",
      "                                         0000  5850.00000 0.00149    0.00983    0.00859   154.287\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.12 0.09 0.08]\n",
      "                                         0000  5925.00000 0.00137    0.00990    0.00859   154.357\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.09 0.09]\n",
      "                                         0000  6000.00000 0.00265    0.00883    0.00859   154.256\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.12 0.08 0.08]\n",
      "                                         0000  6075.00000 0.00193    0.00910    0.00859   154.222\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.13 0.09 0.08]\n",
      "                                         0000  6150.00000 0.00246    0.00976    0.00859   154.096\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.13 0.13 0.09 0.09]\n",
      "                                         0000  6225.00000 0.00298    0.01086    0.00859   154.224\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.1  0.12 0.09 0.09]\n",
      "                                         0000  6300.00000 0.00421    0.01003    0.00859   154.322\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.11 0.09 0.08]\n",
      "                                         0000  6375.00000 0.00123    0.00875    0.00859   154.324\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.11 0.09 0.08]\n",
      "                                         0000  6450.00000 0.00210    0.00810    0.00859   154.215\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.12 0.09 0.09]\n",
      "                                         0000  6525.00000 0.00226    0.00885    0.00810   155.120\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.09 0.08]\n",
      "                                         0000  6600.00000 0.00357    0.00850    0.00810   154.214\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.13 0.09 0.08]\n",
      "                                         0000  6675.00000 0.00153    0.00953    0.00810   154.271\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.09 0.08]\n",
      "                                         0000  6750.00000 0.00270    0.00833    0.00810   154.264\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.08 0.08]\n",
      "                                         0000  6825.00000 0.00170    0.00829    0.00810   154.200\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.12 0.09 0.08]\n",
      "                                         0000  6900.00000 0.00179    0.00909    0.00810   154.172\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.09 0.08]\n",
      "                                         0000  6975.00000 0.00247    0.00839    0.00810   154.090\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.13 0.09 0.08]\n",
      "                                         0000  7050.00000 0.00330    0.00993    0.00810   154.253\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.11 0.09 0.08]\n",
      "                                         0000  7125.00000 0.00160    0.00888    0.00810   154.157\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.14 0.09 0.09]\n",
      "                                         0000  7200.00000 0.00129    0.00967    0.00810   154.404\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.08 0.08]\n",
      "                                         0000  7275.00000 0.00184    0.00816    0.00810   154.079\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.09 0.08]\n",
      "                                         0000  7350.00000 0.00301    0.00917    0.00810   154.200\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.11 0.08 0.08]\n",
      "                                         0000  7425.00000 0.00108    0.00908    0.00810   154.233\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.11 0.12 0.08 0.08]\n",
      "                                         0000  7500.00000 0.00085    0.00966    0.00810   154.336\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.12 0.12 0.08 0.08]\n",
      "                                         0000  7575.00000 0.00074    0.00901    0.00810   154.020\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.11 0.08 0.08]\n",
      "                                         0000  7650.00000 0.00176    0.00832    0.00810   154.264\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.08 0.08]\n",
      "                                         0000  7725.00000 0.00364    0.00862    0.00810   153.845\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.08 0.08]\n",
      "                                         0000  7800.00000 0.00266    0.00909    0.00810   154.305\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.13 0.14 0.09 0.08]\n",
      "                                         0000  7875.00000 0.00499    0.01110    0.00810   154.184\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.11 0.11 0.08 0.07]\n",
      "                                         0000  7950.00000 0.00245    0.00864    0.00810   154.287\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.13 0.09 0.08]\n",
      "                                         0000  8025.00000 0.00313    0.00960    0.00810   154.213\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.08]\n",
      "                                         0000  8100.00000 0.00197    0.00973    0.00810   154.091\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.08 0.08]\n",
      "                                         0000  8175.00000 0.00206    0.00904    0.00810   154.192\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.11 0.12 0.08 0.08]\n",
      "                                         0000  8250.00000 0.00161    0.00914    0.00810   154.200\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.12 0.08 0.08]\n",
      "                                         0000  8325.00000 0.00351    0.00852    0.00810   154.339\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.12 0.08 0.08]\n",
      "                                         0000  8400.00000 0.00133    0.00904    0.00810   154.115\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.12 0.08 0.08]\n",
      "                                         0000  8475.00000 0.00277    0.00841    0.00810   153.997\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.11 0.08 0.08]\n",
      "                                         0000  8550.00000 0.00199    0.00849    0.00810   153.822\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.12 0.08 0.08]\n",
      "                                         0000  8625.00000 0.00625    0.00891    0.00810   154.001\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.12 0.08 0.08]\n",
      "                                         0000  8700.00000 0.00110    0.00912    0.00810   153.862\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.12 0.08 0.08]\n",
      "                                         0000  8775.00000 0.00087    0.00894    0.00810   153.860\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.13 0.08 0.08]\n",
      "                                         0000  8850.00000 0.00193    0.00896    0.00810   154.070\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.11 0.08 0.08]\n",
      "                                         0000  8925.00000 0.00094    0.00845    0.00810   153.972\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.08 0.08]\n",
      "                                         0000  9000.00000 0.00131    0.00793    0.00810   153.812\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.08 0.08]\n",
      "                                         0000  9075.00000 0.00311    0.00840    0.00793   154.864\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.08 0.08]\n",
      "                                         0000  9150.00000 0.00417    0.00879    0.00793   154.122\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.08 0.09]\n",
      "                                         0000  9225.00000 0.00282    0.00882    0.00793   153.975\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.11 0.12 0.08 0.08]\n",
      "                                         0000  9300.00000 0.00856    0.00963    0.00793   154.146\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.09 0.08]\n",
      "                                         0000  9375.00000 0.00186    0.00833    0.00793   154.133\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.12 0.08 0.08]\n",
      "                                         0000  9450.00000 0.00995    0.00873    0.00793   154.116\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.12 0.08 0.08]\n",
      "                                         0000  9525.00000 0.00385    0.00970    0.00793   154.024\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.1  0.08 0.08]\n",
      "                                         0000  9600.00000 0.00088    0.00738    0.00793   154.036\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.08 0.08]\n",
      "                                         0000  9675.00000 0.00252    0.00798    0.00738   155.125\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.12 0.08 0.07]\n",
      "                                         0000  9750.00000 0.00114    0.00768    0.00738   154.268\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.11 0.08 0.08]\n",
      "                                         0000  9825.00000 0.00168    0.00791    0.00738   154.065\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.14 0.08 0.08]\n",
      "                                         0000  9900.00000 0.00325    0.00947    0.00738   154.205\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.09 0.11 0.08 0.08]\n",
      "                                         0000  9975.00000 0.00208    0.00867    0.00738   154.133\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for num_epoch in range(1):\n",
    "    for i in range(num_iterations):\n",
    "        seed += 1 \n",
    "\n",
    "        read_data_batch( X , Y , MAG , max_file_num , 'train', seed)\n",
    "        sess.run(train_step, feed_dict={x: X, y_: Y})\n",
    "\n",
    "        if i%75 == 0:     \n",
    "            train_cost = sess.run(MeanSquareCost, feed_dict={x: X, y_: Y} )\n",
    "            costs.append(train_cost)\n",
    "\n",
    "            sum_rms = 0\n",
    "            eval_cost = 0\n",
    "            chunk_size = 50\n",
    "            num_chunks = int(num_test_samples/chunk_size)\n",
    "            for it in range(num_chunks):\n",
    "                    eval_cost  = eval_cost + sess.run(MeanSquareCost, feed_dict={x: X_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it]], y_: Y_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:]})\n",
    "                    A = sess.run(y_out , feed_dict={ x: X_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it]]})\n",
    "                    B = sess.run(y_out_flipped , feed_dict={ x: X_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it]]})\n",
    "                    Predictions[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:] = get_rotation_corrected(A,B,Y_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:])\n",
    "                    sum_rms = sum_rms + np.std(Predictions[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:]- \\\n",
    "                                               Y_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:],axis=0)\n",
    "            eval_cost = eval_cost / num_chunks\n",
    "            print(\"mod\"+ str(model_num) + \", lr: \" + str(learning_rate) + \", \"  + np.array_str( sum_rms/num_chunks  ,precision=2) )\n",
    "\n",
    "\n",
    "        # show the iteration number, training cost, validation cost, and the average time per iteration for training\n",
    "            print(\"                                         %0.4d  %0.5f %0.5f    %0.5f    %0.5f   %0.3f\"%(num_epoch, i, train_cost,eval_cost,min_eval_cost,(time.time()-start_time)/print_per)) \n",
    "            start_time = time.time()\n",
    "\n",
    "            log_file = open(\"log_file.txt\",\"a\")\n",
    "            log_file.write('%d ' % (num_epoch) + ' '.join(map(str,sum_rms/num_chunks)) + ' %0.5f %0.5f\\n' % (train_cost,eval_cost) )\n",
    "            log_file.close()               \n",
    "\n",
    "            if  SAVE & (eval_cost<min_eval_cost): # save file when validation cost drops\n",
    "                    print(\"saving weights to the disk (eval) ...\")\n",
    "                    save_path = saver.save(sess, save_file)\n",
    "                    print(\"done.\")\n",
    "            min_eval_cost = np.minimum(min_eval_cost,eval_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.12 0.09 0.08]\n",
      "                                         0000  0.00000 0.00257    0.00933    0.00738   25.750\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.11 0.08 0.09]\n",
      "                                         0000  75.00000 0.00235    0.00884    0.00738   155.759\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.08 0.07]\n",
      "                                         0000  150.00000 0.00088    0.00789    0.00738   155.602\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.08 0.07]\n",
      "                                         0000  225.00000 0.00257    0.00779    0.00738   155.429\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.12 0.08 0.08]\n",
      "                                         0000  300.00000 0.00195    0.00868    0.00738   155.254\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.13 0.09 0.08]\n",
      "                                         0000  375.00000 0.00216    0.00995    0.00738   155.301\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.12 0.09 0.08]\n",
      "                                         0000  450.00000 0.00139    0.00850    0.00738   155.168\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.11 0.13 0.09 0.08]\n",
      "                                         0000  525.00000 0.00453    0.00967    0.00738   155.166\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.14 0.08 0.07]\n",
      "                                         0000  600.00000 0.00462    0.01058    0.00738   155.209\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.08 0.08]\n",
      "                                         0000  675.00000 0.00200    0.00875    0.00738   155.045\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.12 0.14 0.09 0.08]\n",
      "                                         0000  750.00000 0.00455    0.01172    0.00738   155.080\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.13 0.08 0.08]\n",
      "                                         0000  825.00000 0.00260    0.00841    0.00738   154.945\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.1  0.13 0.08 0.08]\n",
      "                                         0000  900.00000 0.00177    0.00925    0.00738   155.135\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.13 0.08 0.08]\n",
      "                                         0000  975.00000 0.00447    0.00953    0.00738   154.942\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.1  0.08 0.09]\n",
      "                                         0000  1050.00000 0.00333    0.00843    0.00738   154.923\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.08 0.08]\n",
      "                                         0000  1125.00000 0.00370    0.00722    0.00738   154.853\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.08 0.07]\n",
      "                                         0000  1200.00000 0.00284    0.00813    0.00722   157.084\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.13 0.08 0.08]\n",
      "                                         0000  1275.00000 0.00286    0.00958    0.00722   154.942\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.13 0.08 0.07]\n",
      "                                         0000  1350.00000 0.00456    0.00920    0.00722   154.842\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.08 0.08]\n",
      "                                         0000  1425.00000 0.00151    0.00807    0.00722   154.747\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.12 0.08 0.08]\n",
      "                                         0000  1500.00000 0.00254    0.00908    0.00722   154.933\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.07 0.08]\n",
      "                                         0000  1575.00000 0.00189    0.00777    0.00722   154.781\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.13 0.08 0.08]\n",
      "                                         0000  1650.00000 0.00325    0.00854    0.00722   154.820\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.12 0.12 0.07 0.08]\n",
      "                                         0000  1725.00000 0.00121    0.00903    0.00722   154.788\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.08 0.08]\n",
      "                                         0000  1800.00000 0.00115    0.00868    0.00722   154.574\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.08 0.08]\n",
      "                                         0000  1875.00000 0.00046    0.00745    0.00722   154.625\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.08 0.08]\n",
      "                                         0000  1950.00000 0.00187    0.00791    0.00722   154.644\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.08 0.07]\n",
      "                                         0000  2025.00000 0.00579    0.00877    0.00722   154.700\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.08 0.08]\n",
      "                                         0000  2100.00000 0.00096    0.00825    0.00722   154.775\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.13 0.08 0.07]\n",
      "                                         0000  2175.00000 0.00082    0.00895    0.00722   154.922\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.08 0.08]\n",
      "                                         0000  2250.00000 0.00366    0.00838    0.00722   154.797\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.1  0.08 0.07]\n",
      "                                         0000  2325.00000 0.00117    0.00729    0.00722   154.767\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.13 0.08 0.07]\n",
      "                                         0000  2400.00000 0.00157    0.00820    0.00722   154.818\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.08 0.07]\n",
      "                                         0000  2475.00000 0.00285    0.00770    0.00722   154.742\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.08 0.07]\n",
      "                                         0000  2550.00000 0.00146    0.00701    0.00722   154.747\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.12 0.08 0.07]\n",
      "                                         0000  2625.00000 0.00127    0.00744    0.00701   155.726\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.08 0.07]\n",
      "                                         0000  2700.00000 0.00297    0.00776    0.00701   154.697\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.08 0.07]\n",
      "                                         0000  2775.00000 0.00079    0.00766    0.00701   154.746\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.07 0.07]\n",
      "                                         0000  2850.00000 0.00149    0.00848    0.00701   154.813\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.08 0.07]\n",
      "                                         0000  2925.00000 0.00141    0.00738    0.00701   154.568\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  3000.00000 0.00140    0.00767    0.00701   154.492\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.08 0.08]\n",
      "                                         0000  3075.00000 0.00203    0.00800    0.00701   154.516\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.08 0.08]\n",
      "                                         0000  3150.00000 0.00115    0.00832    0.00701   154.728\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.13 0.07 0.08]\n",
      "                                         0000  3225.00000 0.00099    0.00845    0.00701   154.606\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  3300.00000 0.00244    0.00716    0.00701   154.669\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.13 0.08 0.07]\n",
      "                                         0000  3375.00000 0.00153    0.00850    0.00701   154.615\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  3450.00000 0.00299    0.00713    0.00701   154.447\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.07 0.07]\n",
      "                                         0000  3525.00000 0.00077    0.00764    0.00701   154.625\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.08 0.08]\n",
      "                                         0000  3600.00000 0.00084    0.00707    0.00701   155.172\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.08 0.08]\n",
      "                                         0000  3675.00000 0.00109    0.00724    0.00701   154.792\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  3750.00000 0.00225    0.00677    0.00701   154.666\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.12 0.07 0.07]\n",
      "                                         0000  3825.00000 0.00147    0.00778    0.00677   155.690\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.11 0.08 0.08]\n",
      "                                         0000  3900.00000 0.00174    0.00765    0.00677   154.776\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  3975.00000 0.00193    0.00677    0.00677   154.820\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.08 0.08]\n",
      "                                         0000  4050.00000 0.00110    0.00794    0.00677   155.696\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.08 0.07]\n",
      "                                         0000  4125.00000 0.00624    0.00834    0.00677   154.954\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.08 0.07]\n",
      "                                         0000  4200.00000 0.00085    0.00674    0.00677   154.903\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  4275.00000 0.00112    0.00651    0.00674   156.021\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.09 0.12 0.08 0.07]\n",
      "                                         0000  4350.00000 0.00203    0.00819    0.00651   155.877\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.08 0.11 0.08 0.07]\n",
      "                                         0000  4425.00000 0.00096    0.00710    0.00651   154.917\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.08 0.07]\n",
      "                                         0000  4500.00000 0.00064    0.00735    0.00651   154.726\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.07 0.11 0.07 0.07]\n",
      "                                         0000  4575.00000 0.00114    0.00677    0.00651   154.924\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.1  0.07 0.08]\n",
      "                                         0000  4650.00000 0.00074    0.00746    0.00651   154.965\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  4725.00000 0.00087    0.00635    0.00651   154.855\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.11 0.11 0.07 0.08]\n",
      "                                         0000  4800.00000 0.00374    0.00830    0.00635   155.629\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.1  0.08 0.08]\n",
      "                                         0000  4875.00000 0.00134    0.00747    0.00635   154.711\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.1  0.07 0.08]\n",
      "                                         0000  4950.00000 0.00215    0.00737    0.00635   154.760\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.13 0.11 0.07 0.08]\n",
      "                                         0000  5025.00000 0.00063    0.00926    0.00635   154.781\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  5100.00000 0.00241    0.00763    0.00635   154.770\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.07 0.07]\n",
      "                                         0000  5175.00000 0.00105    0.00792    0.00635   154.816\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.11 0.12 0.08 0.07]\n",
      "                                         0000  5250.00000 0.00660    0.00846    0.00635   154.580\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.11 0.12 0.08 0.07]\n",
      "                                         0000  5325.00000 0.00078    0.00868    0.00635   154.712\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.08 0.1  0.07 0.08]\n",
      "                                         0000  5400.00000 0.00320    0.00733    0.00635   154.726\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.11 0.07 0.07]\n",
      "                                         0000  5475.00000 0.00089    0.00767    0.00635   154.670\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.1  0.07 0.08]\n",
      "                                         0000  5550.00000 0.00142    0.00731    0.00635   154.649\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  5625.00000 0.00098    0.00718    0.00635   154.799\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  5700.00000 0.00063    0.00709    0.00635   154.768\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  5775.00000 0.00130    0.00727    0.00635   154.848\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  5850.00000 0.00364    0.00676    0.00635   154.941\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.07 0.08]\n",
      "                                         0000  5925.00000 0.00111    0.00794    0.00635   154.829\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  6000.00000 0.00087    0.00751    0.00635   154.837\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  6075.00000 0.00151    0.00720    0.00635   154.640\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.07 0.07]\n",
      "                                         0000  6150.00000 0.00116    0.00789    0.00635   154.580\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.08 0.07]\n",
      "                                         0000  6225.00000 0.00195    0.00874    0.00635   154.685\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  6300.00000 0.00032    0.00703    0.00635   154.789\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.12 0.08 0.08]\n",
      "                                         0000  6375.00000 0.00232    0.00753    0.00635   154.750\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  6450.00000 0.00149    0.00736    0.00635   154.574\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.13 0.11 0.08 0.08]\n",
      "                                         0000  6525.00000 0.00154    0.00927    0.00635   154.483\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  6600.00000 0.00048    0.00723    0.00635   154.697\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.12 0.08 0.08]\n",
      "                                         0000  6675.00000 0.00183    0.00866    0.00635   154.656\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.07 0.11 0.07 0.08]\n",
      "                                         0000  6750.00000 0.00127    0.00675    0.00635   154.704\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  6825.00000 0.00100    0.00716    0.00635   154.519\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.08 0.12 0.07 0.07]\n",
      "                                         0000  6900.00000 0.00366    0.00746    0.00635   154.352\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.07 0.07]\n",
      "                                         0000  6975.00000 0.00440    0.00756    0.00635   154.450\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.08]\n",
      "                                         0000  7050.00000 0.00031    0.00742    0.00635   154.342\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.08 0.07]\n",
      "                                         0000  7125.00000 0.00354    0.00716    0.00635   154.708\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  7200.00000 0.00090    0.00701    0.00635   154.799\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.11 0.07 0.07]\n",
      "                                         0000  7275.00000 0.00037    0.00709    0.00635   154.662\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  7350.00000 0.00108    0.00707    0.00635   154.749\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.1  0.07 0.07]\n",
      "                                         0000  7425.00000 0.00299    0.00729    0.00635   154.849\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  7500.00000 0.00052    0.00705    0.00635   154.781\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.08]\n",
      "                                         0000  7575.00000 0.00109    0.00692    0.00635   154.863\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.07 0.08]\n",
      "                                         0000  7650.00000 0.00162    0.00769    0.00635   154.892\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.08 0.07]\n",
      "                                         0000  7725.00000 0.00171    0.00822    0.00635   154.723\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  7800.00000 0.00147    0.00655    0.00635   154.698\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  7875.00000 0.00086    0.00725    0.00635   154.585\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  7950.00000 0.00226    0.00644    0.00635   154.790\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.08 0.11 0.07 0.08]\n",
      "                                         0000  8025.00000 0.00069    0.00690    0.00635   154.772\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.07 0.08]\n",
      "                                         0000  8100.00000 0.00084    0.00780    0.00635   154.643\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  8175.00000 0.00118    0.00667    0.00635   154.679\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  8250.00000 0.00263    0.00707    0.00635   154.654\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  8325.00000 0.00072    0.00699    0.00635   154.696\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  8400.00000 0.00378    0.00711    0.00635   154.709\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  8475.00000 0.00115    0.00642    0.00635   154.455\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.1  0.08 0.08]\n",
      "                                         0000  8550.00000 0.00249    0.00755    0.00635   154.575\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  8625.00000 0.00069    0.00642    0.00635   154.536\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  8700.00000 0.00137    0.00667    0.00635   154.317\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.08 0.07]\n",
      "                                         0000  8775.00000 0.00156    0.00680    0.00635   154.383\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  8850.00000 0.00103    0.00698    0.00635   154.453\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  8925.00000 0.00069    0.00645    0.00635   154.337\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.08 0.07]\n",
      "                                         0000  9000.00000 0.00107    0.00703    0.00635   154.466\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.13 0.08 0.07]\n",
      "                                         0000  9075.00000 0.00212    0.00869    0.00635   154.364\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.12 0.07 0.07]\n",
      "                                         0000  9150.00000 0.00327    0.00689    0.00635   154.498\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  9225.00000 0.00657    0.00658    0.00635   154.554\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  9300.00000 0.00094    0.00700    0.00635   154.320\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  9375.00000 0.00058    0.00668    0.00635   154.554\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  9450.00000 0.00063    0.00697    0.00635   154.445\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  9525.00000 0.00030    0.00675    0.00635   154.452\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.07 0.11 0.08 0.07]\n",
      "                                         0000  9600.00000 0.00082    0.00695    0.00635   155.082\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  9675.00000 0.00072    0.00657    0.00635   154.344\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  9750.00000 0.00217    0.00704    0.00635   154.714\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  9825.00000 0.00166    0.00720    0.00635   154.785\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.07 0.07]\n",
      "                                         0000  9900.00000 0.00084    0.00753    0.00635   154.830\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.08 0.11 0.07 0.07]\n",
      "                                         0000  9975.00000 0.00169    0.00687    0.00635   154.730\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for num_epoch in range(1):\n",
    "    for i in range(num_iterations):\n",
    "        seed += 1 \n",
    "\n",
    "        read_data_batch( X , Y , MAG , max_file_num , 'train', seed)\n",
    "        sess.run(train_step, feed_dict={x: X, y_: Y})\n",
    "\n",
    "        if i%75 == 0:     \n",
    "            train_cost = sess.run(MeanSquareCost, feed_dict={x: X, y_: Y} )\n",
    "            costs.append(train_cost)\n",
    "\n",
    "            sum_rms = 0\n",
    "            eval_cost = 0\n",
    "            chunk_size = 50\n",
    "            num_chunks = int(num_test_samples/chunk_size)\n",
    "            for it in range(num_chunks):\n",
    "                    eval_cost  = eval_cost + sess.run(MeanSquareCost, feed_dict={x: X_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it]], y_: Y_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:]})\n",
    "                    A = sess.run(y_out , feed_dict={ x: X_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it]]})\n",
    "                    B = sess.run(y_out_flipped , feed_dict={ x: X_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it]]})\n",
    "                    Predictions[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:] = get_rotation_corrected(A,B,Y_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:])\n",
    "                    sum_rms = sum_rms + np.std(Predictions[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:]- \\\n",
    "                                               Y_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:],axis=0)\n",
    "            eval_cost = eval_cost / num_chunks\n",
    "            print(\"mod\"+ str(model_num) + \", lr: \" + str(learning_rate) + \", \"  + np.array_str( sum_rms/num_chunks  ,precision=2) )\n",
    "\n",
    "\n",
    "        # show the iteration number, training cost, validation cost, and the average time per iteration for training\n",
    "            print(\"                                         %0.4d  %0.5f %0.5f    %0.5f    %0.5f   %0.3f\"%(num_epoch, i, train_cost,eval_cost,min_eval_cost,(time.time()-start_time)/print_per)) \n",
    "            start_time = time.time()\n",
    "\n",
    "            log_file = open(\"log_file.txt\",\"a\")\n",
    "            log_file.write('%d ' % (num_epoch) + ' '.join(map(str,sum_rms/num_chunks)) + ' %0.5f %0.5f\\n' % (train_cost,eval_cost) )\n",
    "            log_file.close()               \n",
    "\n",
    "            if  SAVE & (eval_cost<min_eval_cost): # save file when validation cost drops\n",
    "                    print(\"saving weights to the disk (eval) ...\")\n",
    "                    save_path = saver.save(sess, save_file)\n",
    "                    print(\"done.\")\n",
    "            min_eval_cost = np.minimum(min_eval_cost,eval_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.08]\n",
      "                                         0000  0.00000 0.00168    0.00658    0.05000   31.507\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.08 0.08]\n",
      "                                         0000  75.00000 0.00332    0.00772    0.00658   156.364\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  150.00000 0.00386    0.00693    0.00658   155.130\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  225.00000 0.00245    0.00670    0.00658   155.201\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  300.00000 0.00362    0.00654    0.00658   154.829\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.12 0.07 0.07]\n",
      "                                         0000  375.00000 0.00034    0.00770    0.00654   155.793\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.1  0.07 0.08]\n",
      "                                         0000  450.00000 0.00111    0.00703    0.00654   154.961\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.13 0.08 0.08]\n",
      "                                         0000  525.00000 0.00740    0.00890    0.00654   154.834\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.08 0.07]\n",
      "                                         0000  600.00000 0.00139    0.00714    0.00654   154.966\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  675.00000 0.00381    0.00649    0.00654   154.928\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.07 0.07]\n",
      "                                         0000  750.00000 0.00129    0.00737    0.00649   155.961\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  825.00000 0.00271    0.00673    0.00649   154.681\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.08 0.07]\n",
      "                                         0000  900.00000 0.00177    0.00669    0.00649   154.856\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  975.00000 0.00095    0.00659    0.00649   154.727\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.11 0.07 0.07]\n",
      "                                         0000  1050.00000 0.00096    0.00734    0.00649   154.616\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.1  0.07 0.07]\n",
      "                                         0000  1125.00000 0.00229    0.00775    0.00649   154.631\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.11 0.07 0.07]\n",
      "                                         0000  1200.00000 0.00217    0.00715    0.00649   154.813\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  1275.00000 0.00119    0.00662    0.00649   154.506\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  1350.00000 0.00180    0.00726    0.00649   154.451\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.1  0.07 0.07]\n",
      "                                         0000  1425.00000 0.00077    0.00760    0.00649   154.483\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.09 0.12 0.07 0.08]\n",
      "                                         0000  1500.00000 0.00459    0.00874    0.00649   154.496\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.08 0.1  0.07 0.07]\n",
      "                                         0000  1575.00000 0.00144    0.00672    0.00649   154.684\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.08 0.1  0.07 0.07]\n",
      "                                         0000  1650.00000 0.00129    0.00676    0.00649   154.273\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  1725.00000 0.00132    0.00697    0.00649   154.500\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.11 0.07 0.07]\n",
      "                                         0000  1800.00000 0.00222    0.00740    0.00649   154.426\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  1875.00000 0.00081    0.00659    0.00649   154.130\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  1950.00000 0.00137    0.00716    0.00649   154.362\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.09 0.08 0.07]\n",
      "                                         0000  2025.00000 0.00077    0.00661    0.00649   154.237\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  2100.00000 0.00197    0.00658    0.00649   154.123\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  2175.00000 0.00122    0.00661    0.00649   154.081\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.12 0.07 0.07]\n",
      "                                         0000  2250.00000 0.00287    0.00712    0.00649   154.340\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.07 0.11 0.08 0.07]\n",
      "                                         0000  2325.00000 0.00087    0.00659    0.00649   153.946\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.08 0.07]\n",
      "                                         0000  2400.00000 0.00164    0.00736    0.00649   153.938\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.09 0.08 0.07]\n",
      "                                         0000  2475.00000 0.00215    0.00681    0.00649   154.254\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.08 0.07]\n",
      "                                         0000  2550.00000 0.00520    0.00811    0.00649   153.976\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  2625.00000 0.00078    0.00674    0.00649   154.324\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.12 0.08 0.08]\n",
      "                                         0000  2700.00000 0.00150    0.00755    0.00649   154.216\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.07 0.1  0.07 0.07]\n",
      "                                         0000  2775.00000 0.00140    0.00626    0.00649   154.225\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.09 0.1  0.07 0.07]\n",
      "                                         0000  2850.00000 0.00046    0.00686    0.00626   155.168\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.08 0.08]\n",
      "                                         0000  2925.00000 0.00473    0.00779    0.00626   154.161\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.07 0.1  0.07 0.07]\n",
      "                                         0000  3000.00000 0.00144    0.00623    0.00626   154.157\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.08 0.08]\n",
      "                                         0000  3075.00000 0.00186    0.00696    0.00623   155.103\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.08 0.08]\n",
      "                                         0000  3150.00000 0.00347    0.00669    0.00623   154.377\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  3225.00000 0.00140    0.00732    0.00623   154.201\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  3300.00000 0.00305    0.00670    0.00623   154.259\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  3375.00000 0.00087    0.00680    0.00623   154.125\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.08 0.08]\n",
      "                                         0000  3450.00000 0.00171    0.00760    0.00623   153.963\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.08]\n",
      "                                         0000  3525.00000 0.00336    0.00747    0.00623   154.676\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  3600.00000 0.00056    0.00605    0.00623   154.255\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.07 0.11 0.07 0.07]\n",
      "                                         0000  3675.00000 0.00035    0.00672    0.00605   155.547\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  3750.00000 0.00061    0.00656    0.00605   154.479\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  3825.00000 0.00049    0.00654    0.00605   154.352\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  3900.00000 0.00159    0.00653    0.00605   154.359\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  3975.00000 0.00100    0.00741    0.00605   154.417\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  4050.00000 0.00294    0.00670    0.00605   154.213\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  4125.00000 0.00087    0.00701    0.00605   154.456\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.07 0.11 0.07 0.07]\n",
      "                                         0000  4200.00000 0.00165    0.00658    0.00605   154.685\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  4275.00000 0.00102    0.00640    0.00605   154.796\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.11 0.07 0.07]\n",
      "                                         0000  4350.00000 0.00100    0.00752    0.00605   154.988\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  4425.00000 0.00072    0.00633    0.00605   154.408\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  4500.00000 0.00110    0.00677    0.00605   154.430\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  4575.00000 0.00054    0.00643    0.00605   154.621\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.08 0.08]\n",
      "                                         0000  4650.00000 0.00060    0.00742    0.00605   154.626\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  4725.00000 0.00152    0.00719    0.00605   154.600\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.08 0.1  0.07 0.07]\n",
      "                                         0000  4800.00000 0.00144    0.00648    0.00605   154.400\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  4875.00000 0.00068    0.00673    0.00605   154.672\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.07 0.11 0.07 0.07]\n",
      "                                         0000  4950.00000 0.00112    0.00651    0.00605   154.654\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.09 0.11 0.07 0.08]\n",
      "                                         0000  5025.00000 0.00105    0.00738    0.00605   154.722\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  5100.00000 0.00076    0.00634    0.00605   154.510\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  5175.00000 0.00156    0.00686    0.00605   154.593\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  5250.00000 0.00117    0.00700    0.00605   154.506\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.08 0.11 0.07 0.07]\n",
      "                                         0000  5325.00000 0.00135    0.00723    0.00605   154.423\n",
      "mod_vgg_16, lr: 0.1, [0.08 0.1  0.11 0.07 0.07]\n",
      "                                         0000  5400.00000 0.00199    0.00883    0.00605   154.666\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.07 0.07]\n",
      "                                         0000  5475.00000 0.00097    0.00797    0.00605   154.334\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  5550.00000 0.00160    0.00752    0.00605   154.309\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.11 0.11 0.07 0.07]\n",
      "                                         0000  5625.00000 0.00134    0.00789    0.00605   154.615\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.07 0.07]\n",
      "                                         0000  5700.00000 0.00291    0.00733    0.00605   154.479\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.11 0.07 0.07]\n",
      "                                         0000  5775.00000 0.00191    0.00743    0.00605   154.220\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.11 0.11 0.07 0.07]\n",
      "                                         0000  5850.00000 0.00110    0.00807    0.00605   154.227\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.08 0.07]\n",
      "                                         0000  5925.00000 0.00226    0.00785    0.00605   154.381\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.09 0.1  0.07 0.07]\n",
      "                                         0000  6000.00000 0.00084    0.00653    0.00605   154.221\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  6075.00000 0.00149    0.00639    0.00605   154.403\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.07]\n",
      "                                         0000  6150.00000 0.00172    0.00621    0.00605   154.343\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.11 0.1  0.07 0.07]\n",
      "                                         0000  6225.00000 0.00085    0.00735    0.00605   154.164\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.09 0.1  0.07 0.07]\n",
      "                                         0000  6300.00000 0.00024    0.00639    0.00605   154.100\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.12 0.07 0.07]\n",
      "                                         0000  6375.00000 0.00183    0.00831    0.00605   154.272\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.07 0.07]\n",
      "                                         0000  6450.00000 0.00259    0.00774    0.00605   153.969\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.08 0.11 0.07 0.07]\n",
      "                                         0000  6525.00000 0.00219    0.00696    0.00605   154.276\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.1  0.07 0.07]\n",
      "                                         0000  6600.00000 0.00194    0.00669    0.00605   154.392\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.1  0.11 0.07 0.07]\n",
      "                                         0000  6675.00000 0.00087    0.00808    0.00605   154.153\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.1  0.07 0.07]\n",
      "                                         0000  6750.00000 0.00077    0.00713    0.00605   153.993\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.1  0.1  0.07 0.07]\n",
      "                                         0000  6825.00000 0.00177    0.00710    0.00605   154.238\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.08 0.1  0.07 0.07]\n",
      "                                         0000  6900.00000 0.00096    0.00595    0.00605   153.960\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  6975.00000 0.00100    0.00653    0.00595   155.112\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.09 0.07 0.06]\n",
      "                                         0000  7050.00000 0.00101    0.00579    0.00595   153.982\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  7125.00000 0.00082    0.00666    0.00579   155.604\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.11 0.1  0.07 0.07]\n",
      "                                         0000  7200.00000 0.00215    0.00715    0.00579   154.550\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.11 0.11 0.08 0.07]\n",
      "                                         0000  7275.00000 0.00274    0.00832    0.00579   154.500\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.1  0.1  0.07 0.06]\n",
      "                                         0000  7350.00000 0.00231    0.00635    0.00579   154.484\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.06]\n",
      "                                         0000  7425.00000 0.00235    0.00631    0.00579   154.684\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.12 0.07 0.06]\n",
      "                                         0000  7500.00000 0.00179    0.00729    0.00579   154.472\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.11 0.07 0.07]\n",
      "                                         0000  7575.00000 0.00061    0.00676    0.00579   154.497\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.12 0.07 0.07]\n",
      "                                         0000  7650.00000 0.00429    0.00738    0.00579   154.466\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.1  0.09 0.07 0.06]\n",
      "                                         0000  7725.00000 0.00030    0.00625    0.00579   154.280\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.11 0.12 0.07 0.07]\n",
      "                                         0000  7800.00000 0.00113    0.00858    0.00579   154.423\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.06]\n",
      "                                         0000  7875.00000 0.00135    0.00637    0.00579   154.541\n",
      "mod_vgg_16, lr: 0.1, [0.07 0.09 0.1  0.07 0.07]\n",
      "                                         0000  7950.00000 0.00263    0.00656    0.00579   154.557\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  8025.00000 0.00095    0.00691    0.00579   154.337\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.1  0.11 0.07 0.06]\n",
      "                                         0000  8100.00000 0.00087    0.00710    0.00579   154.595\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.06]\n",
      "                                         0000  8175.00000 0.00175    0.00642    0.00579   154.182\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.06]\n",
      "                                         0000  8250.00000 0.00126    0.00599    0.00579   154.228\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.06]\n",
      "                                         0000  8325.00000 0.00049    0.00583    0.00579   154.397\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.07 0.09 0.07 0.07]\n",
      "                                         0000  8400.00000 0.00172    0.00521    0.00579   154.172\n",
      "saving weights to the disk (eval) ...\n",
      "done.\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.06 0.06]\n",
      "                                         0000  8475.00000 0.00154    0.00616    0.00521   155.402\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.1  0.06 0.06]\n",
      "                                         0000  8550.00000 0.00122    0.00676    0.00521   154.273\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.06]\n",
      "                                         0000  8625.00000 0.00073    0.00614    0.00521   154.379\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.07 0.09 0.07 0.06]\n",
      "                                         0000  8700.00000 0.00050    0.00545    0.00521   154.144\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.06]\n",
      "                                         0000  8775.00000 0.00117    0.00641    0.00521   154.193\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.06 0.06]\n",
      "                                         0000  8850.00000 0.00159    0.00626    0.00521   154.309\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.1  0.07 0.06]\n",
      "                                         0000  8925.00000 0.00111    0.00692    0.00521   154.486\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.11 0.12 0.07 0.08]\n",
      "                                         0000  9000.00000 0.00186    0.00876    0.00521   154.097\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.07 0.07]\n",
      "                                         0000  9075.00000 0.00057    0.00718    0.00521   154.395\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.12 0.11 0.07 0.08]\n",
      "                                         0000  9150.00000 0.00067    0.00846    0.00521   154.198\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  9225.00000 0.00177    0.00644    0.00521   154.118\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.06 0.06]\n",
      "                                         0000  9300.00000 0.00154    0.00613    0.00521   154.194\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.09 0.07 0.07]\n",
      "                                         0000  9375.00000 0.00104    0.00620    0.00521   154.150\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  9450.00000 0.00121    0.00626    0.00521   153.964\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.08 0.1  0.07 0.07]\n",
      "                                         0000  9525.00000 0.00073    0.00610    0.00521   154.230\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.1  0.11 0.07 0.07]\n",
      "                                         0000  9600.00000 0.00365    0.00696    0.00521   154.194\n",
      "mod_vgg_16, lr: 0.1, [0.05 0.09 0.1  0.07 0.07]\n",
      "                                         0000  9675.00000 0.01004    0.00619    0.00521   154.084\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.08 0.1  0.07 0.06]\n",
      "                                         0000  9750.00000 0.00098    0.00588    0.00521   153.998\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  9825.00000 0.00133    0.00640    0.00521   154.110\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  9900.00000 0.00130    0.00668    0.00521   153.943\n",
      "mod_vgg_16, lr: 0.1, [0.06 0.09 0.1  0.07 0.07]\n",
      "                                         0000  9975.00000 0.00139    0.00702    0.00521   153.952\n"
     ]
    }
   ],
   "source": [
    "seed = 100000\n",
    "print_per = 1\n",
    "\n",
    "costs = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for num_epoch in range(1):\n",
    "    for i in range(num_iterations):\n",
    "        seed += 1 \n",
    "\n",
    "        read_data_batch( X , Y , MAG , max_file_num , 'train', seed)\n",
    "        sess.run(train_step, feed_dict={x: X, y_: Y})\n",
    "\n",
    "        if i%75 == 0:     \n",
    "            train_cost = sess.run(MeanSquareCost, feed_dict={x: X, y_: Y} )\n",
    "            costs.append(train_cost)\n",
    "\n",
    "            sum_rms = 0\n",
    "            eval_cost = 0\n",
    "            chunk_size = 50\n",
    "            num_chunks = int(num_test_samples/chunk_size)\n",
    "            for it in range(num_chunks):\n",
    "                    eval_cost  = eval_cost + sess.run(MeanSquareCost, feed_dict={x: X_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it]], y_: Y_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:]})\n",
    "                    A = sess.run(y_out , feed_dict={ x: X_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it]]})\n",
    "                    B = sess.run(y_out_flipped , feed_dict={ x: X_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it]]})\n",
    "                    Predictions[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:] = get_rotation_corrected(A,B,Y_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:])\n",
    "                    sum_rms = sum_rms + np.std(Predictions[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:]- \\\n",
    "                                               Y_test[ind_t[0+chunk_size*it:chunk_size+chunk_size*it],:],axis=0)\n",
    "            eval_cost = eval_cost / num_chunks\n",
    "            print(\"mod\"+ str(model_num) + \", lr: \" + str(learning_rate) + \", \"  + np.array_str( sum_rms/num_chunks  ,precision=2) )\n",
    "\n",
    "\n",
    "        # show the iteration number, training cost, validation cost, and the average time per iteration for training\n",
    "            print(\"                                         %0.4d  %0.5f %0.5f    %0.5f    %0.5f   %0.3f\"%(num_epoch, i, train_cost,eval_cost,min_eval_cost,(time.time()-start_time)/print_per)) \n",
    "            start_time = time.time()\n",
    "\n",
    "            log_file = open(\"log_file.txt\",\"a\")\n",
    "            log_file.write('%d ' % (num_epoch) + ' '.join(map(str,sum_rms/num_chunks)) + ' %0.5f %0.5f\\n' % (train_cost,eval_cost) )\n",
    "            log_file.close()               \n",
    "\n",
    "            if  SAVE & (eval_cost<min_eval_cost): # save file when validation cost drops\n",
    "                    print(\"saving weights to the disk (eval) ...\")\n",
    "                    save_path = saver.save(sess, save_file)\n",
    "                    print(\"done.\")\n",
    "            min_eval_cost = np.minimum(min_eval_cost,eval_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.4, 0.4, -0.4, 0.4]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And here's the fun part:\n",
    "# plot the recovered parameters (y-axis) against their true values (x-axis)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.plot(Y_test[:,0],Predictions[:,0],' .')\n",
    "plt.plot([0 , 3.5],[0 , 3.5],'--r')\n",
    "plt.xlabel(\"Ein rad (true)\")\n",
    "plt.ylabel(\"Ein rad (predict)\")\n",
    "#plt.axis([0 ,3 ,0 ,3])\n",
    "\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.plot(Y_test[:,1],Predictions[:,1],' .')\n",
    "plt.plot([-1 , 1.],[-1 , 1.],'--r')\n",
    "plt.xlabel(\"elp x (true)\")\n",
    "plt.axis([-1 ,1 ,-1 ,1])\n",
    "\n",
    "\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.plot(Y_test[:,2],Predictions[:,2],' .')\n",
    "plt.plot([-1 , 1.],[-1 , 1.],'--r')\n",
    "plt.xlabel(\"elp y (true)\")\n",
    "plt.axis([-1 ,1 ,-1 ,1])\n",
    "\n",
    "\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.plot(Y_test[:,3],Predictions[:,3],' .')\n",
    "plt.plot([-0.4 , 0.4],[-0.4 , 0.4],'--r')\n",
    "plt.xlabel(\"x (true)\")\n",
    "\n",
    "plt.axis([-0.4 ,0.4 ,-0.4 ,0.4])\n",
    "\n",
    "\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.plot(Y_test[:,4],Predictions[:,4],' .')\n",
    "plt.plot([-0.4 , 0.4],[-0.4 , 0.4],'--r')\n",
    "plt.xlabel(\"y (true)\")\n",
    "plt.axis([-0.4 ,0.4 ,-0.4 ,0.4])\n",
    "\n",
    "plt.savefig('vgg_graphs.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
